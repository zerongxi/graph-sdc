---
env_id: highway-v0
n_envs: &_n_envs 12
gamma: &_gamma 0.9
enable_venv_subprocess: &_enable_venv_subprocess True

env:
  observation:
    type: Kinematics
    features: &_observation_features
      [presence, x, y, vx, vy, cos_h, sin_h]
    features_range: &_observation_features_range
      x: [0, 100]
      y: [0, 100]
      vx: [0, 20]
      vy: [0, 20]
    vehicles_count: 33
    order: sorted
    see_behind: True
    absolute: False
    normalize: True
  action:
    type: DiscreteMetaAction
    target_speeds: [20, 25, 30, 35, 40]
  disable_collision_checks: True
  ego_spacing: 2.0
  duration: 60
  policy_frequency: 2
  simulation_frequency: 5
  vehicles_count: 100
  lanes_count: 6
  vehicles_density: 1.8
  offroad_terminal: True
  reward_speed_range: [20, 40]
  high_speed_reward: 0.8
  right_lane_reward: 0.0
  collision_reward: -0.2

rl_cls: PPO  # {PPO}
PPO:
  model:
    n_steps: 512
    n_epochs: 10
    target_kl: 0.2
    batch_size: 256
    policy: MlpPolicy
    policy_kwargs:
      net_arch: [{pi: [256, 256], vf: [256, 256]}]
    gamma: *_gamma
    # tensorboard_log: tensorboard/MLP_relative
    verbose: 0
  train:
    total_timesteps: 600000
    n_train_envs: *_n_envs
    n_eval_envs: *_n_envs
    eval_timesteps: 10000
    n_eval_episodes: *_n_envs
    lr_init: 0.001
    lr_final: 0.0005
    lr_frac: 0.4

transformer:
  embedding_layer_kwargs:
    in_size: 7
    layer_sizes: [192, 192, 192]
    reshape: False
  attention_layer_kwargs:
    feature_size: 192  # need to be equal to the size of last embedding layer due to implementation
    heads: 2
...
